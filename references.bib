
@incollection{liu_using_2020,
	title = {Using {Machine} {Learning} {Models} to {Predict} {Attrition} in a {Survey} {Panel}},
	isbn = {978-1-118-97635-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118976357.ch14},
	abstract = {Panel attrition occurs when survey participants in the previous wave of the survey stop participating in subsequent surveys. This phenomenon poses challenges to the data quality of panel studies. Therefore, researchers and practitioners are constantly exploring strategies for motivating panel survey participants to remain in the panel and take part in subsequent surveys. The likelihood of attrition varies from one survey respondent to another, so the most effective approach to preventing panel attrition is to find out which respondents are most likely to attrite in the follow-up surveys and focus on getting those respondents to participate. This chapter explores the use machine learning models to predict the likelihood of participating in the follow-up survey of a national telephone panel study, the Surveys of Consumers. Because respondents have already participated in one wave of data collection, more information is available, including responses to survey questions, demographics, and interviewer observation data, that can be used to model participation in future waves. In this study, I use supervised learning techniques to predict the likelihood of participating in the follow-up survey. Specifically, this chapter presents results from four classification models: random forest, support vector machine, LASSO (least absolute shrinkage and selection operator), and logistic regression. Given the complexity of panel attrition and relatively large number of predictors, one might expect that a model relying on only main effects (such as logistic regression) might be underspecified. The other three methods are emerging as popular alternatives to logistic regression. I chose to use them on the basis of their strengths in modeling prediction spaces that are not simply linear. Also, some of the alternative methods allow easy identification of important panel attrition predictors. This study has two goals. First, by comparing different models, I examine whether additional gains in predictive power can be achieved by using machine learning methods rather than traditional logistic regression. Second, the methods used in this study can identify the most important predictors in responsive design or propensity adjustment for panel attrition.},
	language = {en},
	urldate = {2024-03-31},
	booktitle = {Big {Data} {Meets} {Survey} {Science}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Liu, Mingnan},
	year = {2020},
	doi = {10.1002/9781118976357.ch14},
	note = {Section: 14
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118976357.ch14},
	keywords = {classification, Core Literature, Data \& Method, LASSO, machine learning, panel attrition, panel survey, random forest, support vector machine},
	pages = {415--433},
	file = {Full Text PDF:C\:\\Users\\bztan\\Zotero\\storage\\6NNNASXD\\Liu - 2020 - Using Machine Learning Models to Predict Attrition in a Survey Panel.pdf:application/pdf;Snapshot:C\:\\Users\\bztan\\Zotero\\storage\\9FNLA8I4\\9781118976357.html:text/html},
}

@article{kern_predicting_2023,
	title = {Predicting {Nonresponse} in {Future} {Waves} of a {Probability}-{Based} {Mixed}-{Mode} {Panel} with {Machine} {Learning}*},
	volume = {11},
	issn = {2325-0984},
	url = {https://doi.org/10.1093/jssam/smab009},
	doi = {10.1093/jssam/smab009},
	abstract = {Nonresponse in panel studies can lead to a substantial loss in data quality owing to its potential to introduce bias and distort survey estimates. Recent work investigates the usage of machine learning to predict nonresponse in advance, such that predicted nonresponse propensities can be used to inform the data collection process. However, predicting nonresponse in panel studies requires accounting for the longitudinal data structure in terms of model building, tuning, and evaluation. This study proposes a longitudinal framework for predicting nonresponse with machine learning and multiple panel waves and illustrates its application. With respect to model building, this approach utilizes information from multiple waves by introducing features that aggregate previous (non)response patterns. Concerning model tuning and evaluation, temporal crossvalidation is employed by iterating through pairs of panel waves such that the training and test sets move in time. Implementing this approach with data from a German probability-based mixed-mode panel shows that aggregating information over multiple panel waves can be used to build prediction models with competitive and robust performance over all test waves.},
	number = {1},
	urldate = {2024-04-27},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Kern, Christoph and Weiß, Bernd and Kolb, Jan-Philipp},
	month = feb,
	year = {2023},
	pages = {100--123},
	file = {Full Text PDF:C\:\\Users\\bztan\\Zotero\\storage\\HMA2HG8S\\Kern et al. - 2023 - Predicting Nonresponse in Future Waves of a Probability-Based Mixed-Mode Panel with Machine Learning.pdf:application/pdf},
}

@article{rosmann_using_2016,
	title = {Using {Paradata} to {Predict} and {Correct} for {Panel} {Attrition}},
	volume = {34},
	issn = {0894-4393},
	url = {https://doi.org/10.1177/0894439315587258},
	doi = {10.1177/0894439315587258},
	abstract = {This article addresses the questions of whether paradata can help us to improve the models of panel attrition and whether paradata can improve the effectiveness of propensity score weights with respect to reducing attrition biases. The main advantage of paradata is that it is collected as a by-product of the survey process. However, it is still an open question which paradata can be used to model attrition and to what extent these paradata are correlated with the variables of interest. Our analysis used data from a seven-wave web-based panel survey that had been supplemented by three cross-sectional surveys. This split panel design allowed us to assess the magnitude of attrition bias for a large number of substantive variables. Furthermore, this design enabled us to analyze in detail the effectiveness of propensity score weights. Our results showed that some paradata (e.g., response times and participation history) improved the prediction of panel attrition, whereas others did not. In addition, not all the paradata that increased the model fit resulted in weights that effectively reduced bias. These findings highlight the importance of selecting paradata that are linked to both the survey response process and the variables of interest. This article provides a first contribution to this challenge.},
	language = {en},
	number = {3},
	urldate = {2024-04-27},
	journal = {Social Science Computer Review},
	author = {Roßmann, Joss and Gummer, Tobias},
	month = jun,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {312--332},
	file = {SAGE PDF Full Text:C\:\\Users\\bztan\\Zotero\\storage\\TDE3MS3M\\Roßmann and Gummer - 2016 - Using Paradata to Predict and Correct for Panel Attrition.pdf:application/pdf},
}

@article{frankel_looking_2014,
	title = {Looking {Beyond} {Demographics}: {Panel} {Attrition} in the {ANES} and {GSS}: {Political} {Analysis}},
	volume = {22},
	issn = {10471987},
	shorttitle = {Looking {Beyond} {Demographics}},
	url = {http://proxy.lib.umich.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=poh&AN=97037505&site=ehost-live&scope=site},
	doi = {10.1093/pan/mpt020},
	abstract = {Longitudinal or panel surveys offer unique benefits for social science research, but they typically suffer from attrition, which reduces sample size and can result in biased inferences. Previous research tends to focus on the demographic predictors of attrition, conceptualizing attrition propensity as a stable, individual-level characteristic—some individuals (e.g., young, poor, residentially mobile) are more likely to drop out of a study than others. We argue that panel attrition reflects both the characteristics of the individual respondent as well as her survey experience, a factor shaped by the design and implementation features of the study. In this article, we examine and compare the predictors of panel attrition in the 2008–2009 American National Election Study, an online panel, and the 2006–2010 General Social Survey, a face-to-face panel. In both cases, survey experience variables are predictive of panel attrition above and beyond the standard demographic predictors, but the particular measures of relevance differ across the two surveys. The findings inform statistical corrections for panel attrition bias and provide study design insights for future panel data collections.},
	number = {3},
	urldate = {2024-04-27},
	journal = {Political Analysis},
	author = {Frankel, Laura Lazarus and Hillygus, D. Sunshine},
	month = jul,
	year = {2014},
	note = {Publisher: Cambridge University Press},
	keywords = {EXPERIMENTAL philosophy, PANEL analysis, SOCIAL science research, SOCIAL sciences, SOCIAL surveys},
	pages = {336--353},
	file = {EBSCO Full Text:C\:\\Users\\bztan\\Zotero\\storage\\MDX2EE5P\\Frankel and Hillygus - 2014 - Looking Beyond Demographics Panel Attrition in the ANES and GSS Political Analysis.pdf:application/pdf},
}

@article{behr_extent_2005,
	title = {Extent and {Determinants} of {Panel} {Attrition} in the {European} {Community} {Household} {Panel}},
	volume = {21},
	issn = {0266-7215},
	url = {https://www.jstor.org/stable/3559642},
	abstract = {The aim of the paper is to analyse the extent and determinants of panel attrition in the European Community Household Panel (ECHP). The fact that, after five waves, in some countries the response rate has declined to about 50\%, leads to concerns about the representativeness of the remaining participants. We find the extent and determinants of panel attrition to reveal high variability across countries as well as for different waves within one country. Differences were also found when comparing attrition behaviour across different surveys running parallel in the same countries, as was the case for Germany and the United Kingdom (UK). Response rates are found to depend strongly on whether households moved during the sample period and whether the interviewer in the sample period changed. Compared to these two influences, all other characteristics are of minor importance. Despite these different attrition rates, neither is the analysis of income biased, nor is the ranking of national results disturbed.},
	number = {5},
	urldate = {2024-04-27},
	journal = {European Sociological Review},
	author = {Behr, Andreas and Bellgardt, Egon and Rendtel, Ulrich},
	year = {2005},
	note = {Publisher: Oxford University Press},
	pages = {489--512},
	file = {JSTOR Full Text PDF:C\:\\Users\\bztan\\Zotero\\storage\\YXAM74R5\\Behr et al. - 2005 - Extent and Determinants of Panel Attrition in the European Community Household Panel.pdf:application/pdf},
}

@article{minderop_now_2023,
	title = {Now, later, or never? {Using} response-time patterns to predict panel attrition},
	volume = {26},
	issn = {1364-5579},
	shorttitle = {Now, later, or never?},
	url = {https://doi.org/10.1080/13645579.2022.2091259},
	doi = {10.1080/13645579.2022.2091259},
	abstract = {Preventing panel members from attriting is a fundamental challenge for panel surveys. Research has shown that response behavior in earlier waves (response or nonresponse) is a good predictor of panelists’ response behavior in upcoming waves. However, response behavior can be described in greater detail by considering the time until the response is returned. In the present study, we investigated whether respondents who habitually return their survey late and respondents who switch between early and late response in multiple waves are more likely to attrit from a panel. Using data from the GESIS Panel, we found that later response is related to a higher likelihood of attrition (AME = 0.087) and that response-time stability is related to a lower likelihood of attrition (AME = −0.013). Our models predicted most cases of attrition; thus, survey practitioners could potentially predict future attriters by applying these models to their own data.},
	number = {6},
	urldate = {2024-04-27},
	journal = {International Journal of Social Research Methodology},
	author = {Minderop, Isabella and Weiß, Bernd},
	month = nov,
	year = {2023},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13645579.2022.2091259},
	keywords = {data quality, late response, panel survey, paradata, Reluctant response},
	pages = {693--706},
	file = {Full Text PDF:C\:\\Users\\bztan\\Zotero\\storage\\LK5CCCUL\\Minderop and Weiß - 2023 - Now, later, or never Using response-time patterns to predict panel attrition.pdf:application/pdf},
}

@article{minderop_predicting_2023,
	title = {Predicting panel attrition using multiple operationalisations of response time},
	issn = {2296-4754},
	url = {https://surveyinsights.org/?p=18304},
	doi = {10.13094/SMIF-2023-00009},
	abstract = {Panel attrition is a major problem for panel survey infrastructures. When panelists attrit from a panel survey, the infrastructure is faced with (i) the costs of recruiting new respondents, (ii) a broken timeline of existing data, and (iii) potential nonresponse bias. Previous studies have shown that panel attrition can be predicted using respondents’ response time. However, response time has been operationalised in multiple ways, such as (i) the number of days it takes respondents to participate, (ii) the number of contact attempts made by the data collection organisation, and (iii) the proportion of respondents who have participated prior to a given respondent. Due to the different operationalisations of response time, it is challenging to identify the best measurement to use for predicting panel attrition. In the present study, we used data from the GESIS Panel – which is a German probability-based mixed-mode (i.e., web and mail) panel survey – to compare different operationalisations of response time using multiple logistic random-effects models. We found both that the different operationalisations have similar relationships to attrition and that our models correctly predict a similar amount of attrition.},
	language = {en-US},
	urldate = {2024-04-27},
	journal = {Survey Methods: Insights from the Field (SMIF)},
	author = {Minderop, Isabella},
	month = aug,
	year = {2023},
	file = {Minderop - 2023 - Predicting panel attrition using multiple operationalisations of response time.pdf:C\:\\Users\\bztan\\Zotero\\storage\\MT5L93VI\\Minderop - 2023 - Predicting panel attrition using multiple operationalisations of response time.pdf:application/pdf},
}

@article{kocar_power_2023,
	title = {The power of online panel paradata to predict unit nonresponse and voluntary attrition in a longitudinal design},
	volume = {57},
	issn = {1573-7845},
	url = {https://doi.org/10.1007/s11135-022-01385-x},
	doi = {10.1007/s11135-022-01385-x},
	abstract = {The objective of this study is to identify factors affecting participation rates, i.e., nonresponse and voluntary attrition rates, and their predictive power in a probability-based online panel. Participation for this panel had already been investigated in the literature according to the socio-demographic and socio-psychological characteristics of respondents and different types of paradata, such as device type or questionnaire navigation, had also been explored. In this study, the predictive power of online panel participation paradata was instead evaluated, which was expected (at least in theory) to offer even more complex insight into respondents’ behavior over time. This kind of paradata would also enable the derivation of longitudinal variables measuring respondents’ panel activity, such as survey outcome rates and consecutive waves with a particular survey outcome prior to a wave (e.g., response, noncontact, refusal), and could also be used in models controlling for unobserved heterogeneity. Using the Life in Australia™ participation data for all recruited members for the first 30 waves, multiple linear, binary logistic and panel random-effect logit regression analyses were carried out to assess socio-demographic and online panel paradata predictors of nonresponse and attrition that were available and contributed to the accuracy of prediction and the best statistical modeling. The proposed approach with the derived paradata predictors and random-effect logistic regression proved to be reasonably accurate for predicting nonresponse—with just 15 waves of online panel paradata (even without sociodemographics) and logit random-effect modeling almost four out of five nonrespondents could be correctly identified in the subsequent wave.},
	language = {en},
	number = {2},
	urldate = {2024-04-27},
	journal = {Quality \& Quantity},
	author = {Kocar, Sebastian and Biddle, Nicholas},
	month = apr,
	year = {2023},
	keywords = {Online panel paradata, Panel voluntary attrition, Prediction modeling, Random-effect logit model, Unit nonresponse},
	pages = {1055--1078},
	file = {Full Text PDF:C\:\\Users\\bztan\\Zotero\\storage\\C7AYNUVR\\Kocar and Biddle - 2023 - The power of online panel paradata to predict unit nonresponse and voluntary attrition in a longitud.pdf:application/pdf},
}

@article{jankowsky_validation_2022,
	title = {Validation and generalizability of machine learning prediction models on attrition in longitudinal studies},
	volume = {46},
	issn = {0165-0254},
	url = {https://doi.org/10.1177/01650254221075034},
	doi = {10.1177/01650254221075034},
	abstract = {Attrition in longitudinal studies is a major threat to the representativeness of the data and the generalizability of the findings. Typical approaches to address systematic nonresponse are either expensive and unsatisfactory (e.g., oversampling) or rely on the unrealistic assumption of data missing at random (e.g., multiple imputation). Thus, models that effectively predict who most likely drops out in subsequent occasions might offer the opportunity to take countermeasures (e.g., incentives). With the current study, we introduce a longitudinal model validation approach and examine whether attrition in two nationally representative longitudinal panel studies can be predicted accurately. We compare the performance of a basic logistic regression model with a more flexible, data-driven machine learning algorithm—gradient boosting machines. Our results show almost no difference in accuracies for both modeling approaches, which contradicts claims of similar studies on survey attrition. Prediction models could not be generalized across surveys and were less accurate when tested at a later survey wave. We discuss the implications of these findings for survey retention, the use of complex machine learning algorithms, and give some recommendations to deal with study attrition.},
	language = {en},
	number = {2},
	urldate = {2024-04-27},
	journal = {International Journal of Behavioral Development},
	author = {Jankowsky, Kristin and Schroeders, Ulrich},
	month = mar,
	year = {2022},
	note = {Publisher: SAGE Publications Ltd},
	pages = {169--176},
	file = {SAGE PDF Full Text:C\:\\Users\\bztan\\Zotero\\storage\\L7QI2F7Z\\Jankowsky and Schroeders - 2022 - Validation and generalizability of machine learning prediction models on attrition in longitudinal s.pdf:application/pdf},
}

@techreport{surveys_of_consumers_methodological_2024,
	title = {Methodological {Improvements} {Begin} with {April} 2024 {Preliminary} {Release}},
	url = {https://data.sca.isr.umich.edu/fetchdoc.php?docid=75436},
	author = {Surveys of Consumers,},
	year = {2024},
}

@techreport{surveys_of_consumers_survey_2024,
	title = {Survey {Description}},
	url = {https://data.sca.isr.umich.edu/fetchdoc.php?docid=24774},
	author = {Surveys of Consumers,},
	year = {2024},
}


@article{lynn_panel_2014,
	title = {Panel {Attrition}: {How} {Important} is {Interviewer} {Continuity}?},
	volume = {30},
	issn = {0282-423X},
	shorttitle = {Panel {Attrition}},
	url = {https://journals.sagepub.com/doi/abs/10.2478/jos-2014-0028},
	doi = {10.2478/jos-2014-0028},
	abstract = {We assess whether the probability of a sample member cooperating at a particular wave of a panel survey is greater if the same interviewer is deployed as at the previous wave. Previous research on this topic mainly uses nonexperimental data. Consequently, a) interviewer change is generally nonrandom, and b) continuing interviewers are more experienced by the time of the next wave. Our study is based on a balanced experiment in which both interviewer continuity and experience are controlled. Multilevel multiple membership models are used to explore the effects of interviewer continuity on refusal rate as well as interactions of interviewer continuity with other variables. We find that continuity reduces refusal propensity for younger respondents but not for older respondents, and that this effect depends on the age of the interviewer. This supports the notion that interviewer continuity may be beneficial in some situations, but not necessarily in others.},
	language = {en},
	number = {3},
	urldate = {2024-04-27},
	journal = {Journal of Official Statistics},
	author = {Lynn, Peter and Kaminska, Olena and Goldstein, Harvey},
	month = sep,
	year = {2014},
	note = {Publisher: SAGE Publications},
	pages = {443--457},
	file = {SAGE PDF Full Text:C\:\\Users\\bztan\\Zotero\\storage\\LF56KZ54\\Lynn et al. - 2014 - Panel Attrition How Important is Interviewer Continuity.pdf:application/pdf},
}

@article{lynn_alternative_2013,
	title = {Alternative {Sequential} {Mixed}-{Mode} {Designs}: {Effects} on {Attrition} {Rates}, {Attrition} {Bias}, and {Costs}},
	volume = {1},
	issn = {2325-0984},
	shorttitle = {Alternative {Sequential} {Mixed}-{Mode} {Designs}},
	url = {https://doi.org/10.1093/jssam/smt015},
	doi = {10.1093/jssam/smt015},
	abstract = {This article considers the effect that a wave of mixed-mode data collection (telephone and face-to-face), in an otherwise face-to-face survey, can have on panel attrition and the extent to which this effect is dependent on the nature of the mode-switch protocol. Findings are reported from an experiment carried out at wave 2 of the UK Household Longitudinal Study Innovation Panel, a survey in which the objective is to interview each adult member of the household. One protocol involves making intensive efforts to interview each household member by telephone before switching to face-to-face, and the other involves switching a household to face-to-face as soon as it is apparent that an interviewer visit will be needed for at least one household member. I assess effects on response at waves 2, 3, and 4. With both protocols response rate is lower at wave 2 than with face-to-face single-mode data collection, but with the protocol involving intensive efforts, this response differential is eroded by wave 4, whereas with the other protocol the difference remains. This difference in effect may be caused by intra-household communications.},
	number = {2},
	urldate = {2024-04-27},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Lynn, Peter},
	month = nov,
	year = {2013},
	pages = {183--205},
}


@article{fitzgerald_analysis_1998,
	title = {An {Analysis} of {Sample} {Attrition} in {Panel} {Data}: {The} {Michigan} {Panel} {Study} of {Income} {Dynamics}},
	volume = {33},
	issn = {0022-166X},
	shorttitle = {An {Analysis} of {Sample} {Attrition} in {Panel} {Data}},
	url = {https://www.jstor.org/stable/146433},
	doi = {10.2307/146433},
	abstract = {By 1989 the Michigan Panel Study on Income Dynamics (PSID) had experienced approximately 50 percent sample loss from cumulative attrition from its initial 1968 membership. We study the effect of this attrition on the unconditional distributions of several socioeconomic variables and on the estimates of several sets of regression coefficients. We provide a statistical framework for conducting tests for attrition bias that draws a sharp distinction between selection on unobservables and on observables and that shows that weighted least squares can generate consistent parameter estimates when selection is based on observables, even when they are endogenous. Our empirical analysis shows that attrition is highly selective and is concentrated among lower socioeconomic status individuals. We also show that attrition is concentrated among those with more unstable earnings, marriage, and migration histories. Nevertheless, we find that these variables explain very little of the attrition in the sample, and that the selection that occurs is moderated by regression-to-the-mean effects from selection on transitory components that fade over time. Consequently, despite the large amount of attrition, we find no strong evidence that attrition has seriously distorted the representativeness of the PSID through 1989, and considerable evidence that its cross-sectional representativeness has remained roughly intact.},
	number = {2},
	urldate = {2024-04-29},
	journal = {The Journal of Human Resources},
	author = {Fitzgerald, John and Gottschalk, Peter and Moffitt, Robert},
	year = {1998},
	note = {Publisher: [University of Wisconsin Press, Board of Regents of the University of Wisconsin System]},
	pages = {251--299},
	file = {JSTOR Full Text PDF:C\:\\Users\\bztan\\Zotero\\storage\\M2XA2DAI\\Fitzgerald et al. - 1998 - An Analysis of Sample Attrition in Panel Data The Michigan Panel Study of Income Dynamics.pdf:application/pdf},
}

